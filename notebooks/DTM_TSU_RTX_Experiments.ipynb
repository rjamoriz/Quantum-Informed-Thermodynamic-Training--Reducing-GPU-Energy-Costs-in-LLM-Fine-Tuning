{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f09e6e4",
   "metadata": {},
   "source": [
    "# \ud83c\udf21\ufe0f Workshop: Thermodynamic-Regularized Training for Energy-Aware LLM Fine-Tuning\n",
    "\n",
    "<div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 30px; border-radius: 15px; color: white; text-align: center; margin: 20px 0;\">\n",
    "  <h2 style=\"margin: 0; font-size: 28px;\">Reducing GPU Energy Costs in LLM Fine-Tuning</h2>\n",
    "  <h3 style=\"margin: 10px 0; font-size: 20px; font-weight: normal;\">Entropy-Regularized Optimization + Real Energy Monitoring</h3>\n",
    "  <p style=\"margin: 15px 0; font-size: 14px; opacity: 0.9;\">Inspired by thermodynamic computing ideas (e.g., Extropic, 2024)</p>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udfaf Workshop Objectives\n",
    "\n",
    "By the end of this workshop, you will:\n",
    "\n",
    "1. **Understand** how entropy regularization changes the optimization landscape\n",
    "2. **Implement** a Thermodynamic Sampling Unit (TSU) as stochastic weights\n",
    "3. **Measure** GPU energy consumption during training (NVML)\n",
    "4. **Compare** baseline SGD vs. free-energy-style objectives\n",
    "5. **Analyze** energy\u2013performance trade-offs with clear metrics\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83e\udded What This Notebook *Is* (and Is Not)\n",
    "\n",
    "- **Is:** A practical, hypothesis-driven experiment on a small GPT model\n",
    "- **Is not:** A guarantee of energy savings (results depend on hardware + settings)\n",
    "- **Scope:** GPU training, TSU stochastic weights, energy measurements, and careful comparison\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udfdb\ufe0f Training Paradigms (Reality vs. Optional Concepts)\n",
    "\n",
    "### **Paradigm 1: Classical Supervised Learning**\n",
    "$$\\min_\\theta \\; \\mathcal{L}(\\theta) = \\mathbb{E}_{(x,y)\\sim\\mathcal{D}}[\\ell(f_\\theta(x), y)]$$\n",
    "\n",
    "### **Paradigm 2: Entropy-Regularized Training (This Notebook)**\n",
    "$$\\min_{q(\\theta)} \\; \\mathbb{E}_{\\theta\\sim q}[\\mathcal{L}(\\theta)] - T \\cdot S(q) + \\lambda D_{KL}[q||p]$$\n",
    "\n",
    "where:\n",
    "- $\\mathcal{L}(\\theta)$: Standard loss function (cross-entropy)\n",
    "- $S(q)$: Entropy of the parameter distribution (exploration pressure)\n",
    "- $T$: Temperature (exploration\u2013exploitation trade-off)\n",
    "- $D_{KL}[q||p]$: Regularization toward a prior\n",
    "\n",
    "### **Paradigm 3: QPU Optimization (Optional Demo)**\n",
    "Included as a **standalone illustration** (not integrated into training results).\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcca Intended Outcomes (Hypotheses)\n",
    "\n",
    "- **Energy Efficiency:** Potential reduction in energy per unit loss improvement\n",
    "- **Training Stability:** Smoother gradients from stochastic exploration\n",
    "- **Generalization:** Mild improvements in validation loss under some regimes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bf1b91",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83d\udcd0 Mathematical Framework\n",
    "\n",
    "### **1. Classical Objective Function**\n",
    "\n",
    "Standard supervised learning minimizes empirical risk:\n",
    "\n",
    "$$\\mathcal{L}(\\theta) = \\frac{1}{N}\\sum_{i=1}^{N} \\ell(f_\\theta(x_i), y_i)$$\n",
    "\n",
    "**For language modeling:**\n",
    "$$\\mathcal{L}(\\theta) = -\\frac{1}{T}\\sum_{t=1}^{T} \\log P_\\theta(x_t | x_{<t})$$\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Thermodynamic Reformulation: Free Energy / Variational Objective**\n",
    "\n",
    "We treat parameters as random variables with a factorized Gaussian:\n",
    "$$q(\\theta) = \\mathcal{N}(\\mu, \\mathrm{diag}(\\sigma^2))$$\n",
    "\n",
    "We optimize a **free-energy-style** objective:\n",
    "\n",
    "$$\\boxed{\\mathcal{F}(q) = \\mathbb{E}_{q(\\theta)}[\\mathcal{L}(\\theta)] - T \\cdot S(q) + \\lambda D_{KL}[q||p]}$$\n",
    "\n",
    "This connects to variational Bayes:\n",
    "\n",
    "$$\\mathbb{E}_{q}[\\mathcal{L}] + \\lambda D_{KL}[q||p] \\quad \\text{(ELBO-style)}$$\n",
    "\n",
    "**Key intuition:**\n",
    "- The loss term pulls parameters toward accuracy.\n",
    "- The entropy term pushes toward exploration (wider distributions).\n",
    "- The KL term keeps the distribution grounded in a prior.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Entropy and KL for Gaussian Weights**\n",
    "\n",
    "**Differential Entropy:**\n",
    "$$S(q) = \\frac{1}{2}\\sum_{i=1}^{d} \\left(1 + \\log(2\\pi\\sigma_i^2)\\right)$$\n",
    "\n",
    "**KL to standard normal prior:**\n",
    "$$D_{KL} = \\frac{1}{2}\\sum_{i=1}^{d}\\left(\\mu_i^2 + \\sigma_i^2 - \\log(\\sigma_i^2) - 1\\right)$$\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Attention Entropy (Monitoring Only)**\n",
    "\n",
    "We also track **attention entropy** for interpretability:\n",
    "\n",
    "$$H(A_i) = -\\sum_{j=1}^{T} A_{ij}\\log A_{ij}$$\n",
    "\n",
    "This is **not part of the objective** here, but helps interpret training dynamics.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. GPU Energy Consumption Model**\n",
    "\n",
    "Total training energy:\n",
    "\n",
    "$$E_{total} = \\int_{0}^{T_{train}} P(t) \\, dt \\approx \\sum_{i=1}^{N_{steps}} P_i \\cdot \\Delta t_i$$\n",
    "\n",
    "Measured via NVIDIA NVML: `nvmlDeviceGetPowerUsage()`\n",
    "\n",
    "**Energy efficiency metric:**\n",
    "$$\\eta = \\frac{\\Delta \\mathcal{L}}{E_{total}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41fbc34",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83d\ude80 Experimental Pipeline\n",
    "\n",
    "```ascii\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502          ENTROPY-REGULARIZED GPU WORKFLOW                   \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "\n",
    "  STEP 1: Environment Setup & GPU Verification\n",
    "     \u251c\u2500 \u2705 Verify CUDA/RTX availability\n",
    "     \u251c\u2500 \u26a1 Initialize NVML energy monitoring\n",
    "     \u2514\u2500 \ud83d\udce6 Install: PyTorch, pynvml\n",
    "\n",
    "  STEP 2: Data Preparation\n",
    "     \u251c\u2500 \ud83d\udcda Load Tiny Shakespeare (character-level)\n",
    "     \u251c\u2500 \ud83d\udd24 Build vocabulary + tokenizer\n",
    "     \u2514\u2500 \ud83d\udcca Train/val splits (90/10)\n",
    "\n",
    "  STEP 3: Model Architecture\n",
    "     \u251c\u2500 \ud83c\udfd7\ufe0f Minimal GPT (Transformer blocks)\n",
    "     \u251c\u2500 \ud83d\udc41\ufe0f Causal self-attention + entropy tracking\n",
    "     \u2514\u2500 \ud83d\udccf ~1-2M parameters (laptop-friendly)\n",
    "\n",
    "  STEP 4: Baseline Training (Classical)\n",
    "     \u251c\u2500 \ud83d\udcc9 Standard cross-entropy minimization\n",
    "     \u251c\u2500 \u23f1\ufe0f Measure: time, energy (J), final loss\n",
    "     \u2514\u2500 \ud83d\udcca Establish performance baseline\n",
    "\n",
    "  STEP 5: TSU Free-Energy Training\n",
    "     \u251c\u2500 \ud83c\udf21\ufe0f Stochastic weights via TSU linear layers\n",
    "     \u251c\u2500 \ud83d\udd04 Optimize: L - T\u00b7S + \u03bb\u00b7KL\n",
    "     \u251c\u2500 \ud83d\udcc8 Track: loss, free energy, entropy evolution\n",
    "     \u2514\u2500 \u26a1 Compare energy efficiency vs. baseline\n",
    "\n",
    "  STEP 6: Optional QPU Demo (Standalone)\n",
    "     \u251c\u2500 \ud83d\udd2e PennyLane QAOA circuit example\n",
    "     \u2514\u2500 \ud83e\uddea Demonstration only (not in training loop)\n",
    "\n",
    "  STEP 7: Comparative Analysis\n",
    "     \u251c\u2500 \ud83d\udcca Baseline vs. TSU\n",
    "     \u251c\u2500 \u26a1 Energy consumption analysis\n",
    "     \u251c\u2500 \ud83c\udfaf Training stability metrics\n",
    "     \u2514\u2500 \ud83d\udca1 Efficiency gains report\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad92963c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83d\udee0\ufe0f STEP 1: Environment Setup & GPU Verification\n",
    "\n",
    "**Goal:** Confirm GPU availability and enable energy monitoring.\n",
    "\n",
    "We do *not* assume any fixed speedup factor. Instead, we measure actual training time and energy on your hardware.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db7f6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from typing import Optional, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check CUDA availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\ud83c\udfae Device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\u2705 GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"\ud83d\udd22 CUDA Capability: {torch.cuda.get_device_capability(0)}\")\n",
    "    print(f\"\ud83d\udcbe Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  CPU mode - GPU not available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297fac5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NVML Energy Monitoring\n",
    "try:\n",
    "    import pynvml\n",
    "    pynvml.nvmlInit()\n",
    "    GPU_MONITORING = True\n",
    "    print(\"\u2705 NVML initialized - Energy monitoring available\")\n",
    "except:\n",
    "    GPU_MONITORING = False\n",
    "    print(\"\u26a0\ufe0f  pynvml not available - Install with: pip install pynvml\")\n",
    "\n",
    "class NVMLPowerMeter:\n",
    "    \"\"\"Real-time GPU power measurement using NVIDIA Management Library\"\"\"\n",
    "    def __init__(self, device_idx=0):\n",
    "        if not GPU_MONITORING:\n",
    "            raise RuntimeError(\"pynvml not available\")\n",
    "        self.handle = pynvml.nvmlDeviceGetHandleByIndex(device_idx)\n",
    "        self.measurements = []\n",
    "        \n",
    "    def start(self):\n",
    "        self.measurements = []\n",
    "        self.start_time = time.time()\n",
    "        return self\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Get instantaneous power (Watts)\"\"\"\n",
    "        power_mw = pynvml.nvmlDeviceGetPowerUsage(self.handle)\n",
    "        power_w = power_mw / 1000.0\n",
    "        self.measurements.append((time.time(), power_w))\n",
    "        return power_w\n",
    "    \n",
    "    def stop(self) -> dict:\n",
    "        \"\"\"Calculate total energy consumed (Joules)\"\"\"\n",
    "        if len(self.measurements) < 2:\n",
    "            return {'energy_j': 0, 'avg_power_w': 0, 'duration_s': 0}\n",
    "        \n",
    "        total_energy = 0\n",
    "        for i in range(len(self.measurements)-1):\n",
    "            t1, p1 = self.measurements[i]\n",
    "            t2, p2 = self.measurements[i+1]\n",
    "            dt = t2 - t1\n",
    "            avg_power = (p1 + p2) / 2\n",
    "            total_energy += avg_power * dt\n",
    "        \n",
    "        duration = self.measurements[-1][0] - self.measurements[0][0]\n",
    "        avg_power = total_energy / duration if duration > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'energy_j': total_energy,\n",
    "            'avg_power_w': avg_power,\n",
    "            'duration_s': duration,\n",
    "            'peak_power_w': max(p for _, p in self.measurements)\n",
    "        }\n",
    "\n",
    "print(\"\u26a1 NVMLPowerMeter class loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d050d666",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83c\udf21\ufe0f STEP 2: Thermodynamic Sampling Unit (TSU) Implementation\n",
    "\n",
    "**Key Idea:** Replace selected linear layers with *stochastic weights*.\n",
    "\n",
    "We model each weight as:\n",
    "$$w_i \\sim \\mathcal{N}(\\mu_i, \\sigma_i^2)$$\n",
    "\n",
    "**Reparameterized sampling (differentiable):**\n",
    "$$w_i = \\mu_i + \\sigma_i \\cdot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0,1)$$\n",
    "\n",
    "**Entropy term (Gaussian):**\n",
    "$$S(q) = \\frac{1}{2}\\sum_i \\left(1 + \\log(2\\pi\\sigma_i^2)\\right)$$\n",
    "\n",
    "**Training objective:**\n",
    "$$\\mathcal{F} = \\mathcal{L} - T \\cdot S(q) + \\lambda D_{KL}[q||p]$$\n",
    "\n",
    "This creates a controlled exploration pressure on the parameters. If entropy weight is too high, the model can inflate variance without improving loss, so we keep it small and monitor convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc9ad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class TSULinear(nn.Module):\n",
    "    '''\n",
    "    Stochastic linear layer using Gaussian weight distributions.\n",
    "    Reparameterization keeps gradients flowing to (mu, log_var).\n",
    "    '''\n",
    "    def __init__(self, in_features, out_features, temperature=1.0, bias=True,\n",
    "                 log_var_init=-5.0, log_var_min=-10.0, log_var_max=2.0):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.temperature = temperature\n",
    "        self.log_var_min = log_var_min\n",
    "        self.log_var_max = log_var_max\n",
    "\n",
    "        # Mean and log-variance for weights\n",
    "        self.weight_mean = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.weight_log_var = nn.Parameter(torch.full((out_features, in_features), log_var_init))\n",
    "\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "        # Initialize means like a standard Linear layer\n",
    "        nn.init.kaiming_uniform_(self.weight_mean, a=math.sqrt(5))\n",
    "\n",
    "    def _clamped_log_var(self):\n",
    "        return torch.clamp(self.weight_log_var, self.log_var_min, self.log_var_max)\n",
    "\n",
    "    def sample_weight(self):\n",
    "        log_var = self._clamped_log_var()\n",
    "        std = torch.exp(0.5 * log_var) * self.temperature\n",
    "        eps = torch.randn_like(std)\n",
    "        return self.weight_mean + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Use stochastic weights during training, mean weights during eval\n",
    "        w = self.sample_weight() if self.training else self.weight_mean\n",
    "        return F.linear(x, w, self.bias)\n",
    "\n",
    "    def entropy(self):\n",
    "        log_var = self._clamped_log_var()\n",
    "        return 0.5 * torch.sum(1.0 + log_var + math.log(2.0 * math.pi))\n",
    "\n",
    "    def kl_divergence(self):\n",
    "        log_var = self._clamped_log_var()\n",
    "        return -0.5 * torch.sum(1.0 + log_var - self.weight_mean.pow(2) - log_var.exp())\n",
    "\n",
    "print(\"\ud83c\udf21\ufe0f  TSULinear class loaded\")\n",
    "print(\"   - Gaussian weight sampling with reparameterization\")\n",
    "print(\"   - Entropy and KL available for free-energy objectives\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453af237",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83c\udfd7\ufe0f STEP 3: Model Architecture - Minimal GPT with Attention Entropy Tracking\n",
    "\n",
    "**Key Points:**\n",
    "- We use a small GPT to keep experiments tractable.\n",
    "- Attention entropy is logged for interpretability (not optimized directly).\n",
    "- TSU is injected by swapping a linear layer with `TSULinear` in attention.\n",
    "\n",
    "### **Self-Attention Mechanism:**\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "### **Attention Entropy (Monitoring):**\n",
    "\n",
    "$$H(A_i) = -\\sum_{j=1}^{T} A_{ij} \\log A_{ij}$$\n",
    "\n",
    "### **Causal Masking:**\n",
    "\n",
    "$$A_{ij} = \\begin{cases}\n",
    "\\frac{\\exp(q_i \\cdot k_j / \\sqrt{d_k})}{\\sum_{j'\\leq i}\\exp(q_i \\cdot k_{j'} / \\sqrt{d_k})} & j \\leq i \\\\\n",
    "0 & j > i\n",
    "\\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491b6384",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    '''\n",
    "    Causal self-attention with entropy tracking.\n",
    "    Optionally uses TSULinear for stochastic attention projections.\n",
    "    '''\n",
    "    def __init__(self, n_embd: int, n_head: int, block_size: int, dropout: float = 0.1,\n",
    "                 use_tsu: bool = False, tsu_temperature: float = 1.0):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.n_embd = n_embd\n",
    "        self.dropout = dropout\n",
    "        self.use_tsu = use_tsu\n",
    "\n",
    "        # Key, Query, Value projections\n",
    "        if use_tsu:\n",
    "            self.c_attn = TSULinear(n_embd, 3 * n_embd, temperature=tsu_temperature)\n",
    "        else:\n",
    "            self.c_attn = nn.Linear(n_embd, 3 * n_embd)\n",
    "        self.c_proj = nn.Linear(n_embd, n_embd)\n",
    "\n",
    "        # Regularization\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Causal mask\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(block_size, block_size))\n",
    "                            .view(1, 1, block_size, block_size))\n",
    "\n",
    "        # Track attention entropy (for thermodynamic analysis)\n",
    "        self.last_attn_entropy = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()  # Batch, Sequence length, Embedding dim\n",
    "\n",
    "        # Calculate Q, K, V\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        # Attention scores\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / np.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "\n",
    "        # Compute attention entropy: H(p) = -\u03a3 p\u00b7log(p)\n",
    "        att_entropy = -(att * torch.log(att + 1e-10)).sum(dim=-1).mean()\n",
    "        self.last_attn_entropy = att_entropy.item()\n",
    "\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v  # (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        return self.resid_dropout(self.c_proj(y))\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    '''Transformer block with attention + MLP'''\n",
    "    def __init__(self, n_embd: int, n_head: int, block_size: int, dropout: float = 0.1,\n",
    "                 use_tsu: bool = False, tsu_temperature: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.attn = CausalSelfAttention(n_embd, n_head, block_size, dropout,\n",
    "                                        use_tsu=use_tsu, tsu_temperature=tsu_temperature)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class TinyGPT(nn.Module):\n",
    "    '''\n",
    "    Minimal GPT-style language model\n",
    "    ~1-2M parameters (laptop-friendly)\n",
    "    '''\n",
    "    def __init__(self, vocab_size: int, block_size: int = 256,\n",
    "                 n_embd: int = 384, n_head: int = 6, n_layer: int = 6, dropout: float = 0.1,\n",
    "                 use_tsu: bool = False, tsu_temperature: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "\n",
    "        self.transformer = nn.ModuleDict({\n",
    "            'wte': nn.Embedding(vocab_size, n_embd),  # Token embeddings\n",
    "            'wpe': nn.Embedding(block_size, n_embd),  # Position embeddings\n",
    "            'drop': nn.Dropout(dropout),\n",
    "            'h': nn.ModuleList([TransformerBlock(n_embd, n_head, block_size, dropout,\n",
    "                                                use_tsu=use_tsu, tsu_temperature=tsu_temperature)\n",
    "                               for _ in range(n_layer)]),\n",
    "            'ln_f': nn.LayerNorm(n_embd)\n",
    "        })\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "\n",
    "        # Weight tying\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.block_size, f\"Sequence length {t} exceeds block_size {self.block_size}\"\n",
    "\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "        # Forward pass\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def get_num_params(self):\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "\n",
    "print(\"\ud83c\udfd7\ufe0f  TinyGPT model architecture loaded\")\n",
    "print(\"   - Optional TSU linear layers in attention\")\n",
    "print(\"   - Causal self-attention with entropy tracking\")\n",
    "print(\"   - Configurable depth: n_layer, n_embd, n_head\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2016b6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83d\udcda STEP 4: Data Preparation - Tiny Shakespeare Dataset\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "\n",
    "### **Character-Level Language Modeling:**\n",
    "\n",
    "Given a sequence $x = (x_1, ..., x_T)$ where $x_t \\in \\mathcal{V}$ (vocabulary):\n",
    "\n",
    "$$P(x) = \\prod_{t=1}^{T} P(x_t | x_{<t})$$\n",
    "\n",
    "### **Cross-Entropy Loss:**\n",
    "\n",
    "$$\\mathcal{L} = -\\frac{1}{T}\\sum_{t=1}^{T} \\log P_\\theta(x_t | x_{<t}) = -\\frac{1}{T}\\sum_{t=1}^{T} \\sum_{v \\in \\mathcal{V}} \\mathbb{1}[x_t = v] \\log P_\\theta(v | x_{<t})$$\n",
    "\n",
    "### **Perplexity:**\n",
    "\n",
    "$$\\text{PPL} = \\exp(\\mathcal{L}) = \\exp\\left(-\\frac{1}{T}\\sum_{t=1}^{T}\\log P_\\theta(x_t | x_{<t})\\right)$$\n",
    "\n",
    "Lower perplexity = better model.\n",
    "\n",
    "### **Dataset Statistics:**\n",
    "\n",
    "- Total tokens: $N \\approx 1.1M$\n",
    "- Vocabulary size: $|\\mathcal{V}| = 65$ (characters)\n",
    "- Train/Val split: $90\\% / 10\\%$\n",
    "- Context window: $T_{ctx} = 128$ tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269e4685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "# Download Tiny Shakespeare\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "print(\"\ud83d\udce5 Downloading Tiny Shakespeare...\")\n",
    "with urllib.request.urlopen(url) as response:\n",
    "    text = response.read().decode('utf-8')\n",
    "\n",
    "print(f\"\u2705 Downloaded {len(text):,} characters\")\n",
    "print(f\"\ud83d\udcd6 Preview:\\n{text[:200]}...\")\n",
    "\n",
    "# Build vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "print(f\"\\n\ud83d\udd24 Vocabulary size: {vocab_size}\")\n",
    "print(f\"   Characters: {''.join(chars[:20])}...\")\n",
    "\n",
    "# Train/val split\n",
    "n = len(text)\n",
    "train_data = torch.tensor(encode(text[:int(0.9*n)]), dtype=torch.long)\n",
    "val_data = torch.tensor(encode(text[int(0.9*n):]), dtype=torch.long)\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Dataset splits:\")\n",
    "print(f\"   Train: {len(train_data):,} tokens\")\n",
    "print(f\"   Val:   {len(val_data):,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd3abb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    \"\"\"Character-level dataset with sliding window\"\"\"\n",
    "    def __init__(self, data, block_size):\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.data[idx:idx + self.block_size + 1]\n",
    "        x = chunk[:-1]\n",
    "        y = chunk[1:]\n",
    "        return x, y\n",
    "\n",
    "# Create dataloaders\n",
    "BLOCK_SIZE = 128\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = CharDataset(train_data, BLOCK_SIZE)\n",
    "val_dataset = CharDataset(val_data, BLOCK_SIZE)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                         num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                       num_workers=0, pin_memory=True)\n",
    "\n",
    "print(f\"\u2705 DataLoaders created:\")\n",
    "print(f\"   Block size: {BLOCK_SIZE}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Train batches: {len(train_loader)}\")\n",
    "print(f\"   Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f770b0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83d\udd2c STEP 5: Training Functions - Baseline vs. TSU\n",
    "\n",
    "**Optimization Logic (Important):**\n",
    "\n",
    "- **Baseline** minimizes only loss: $\\mathcal{L}(\\theta)$.\n",
    "- **TSU training** minimizes a *free-energy-like* objective:\n",
    "\n",
    "$$\\mathcal{F} = \\mathcal{L} - T\\cdot S(q) + \\lambda D_{KL}[q||p]$$\n",
    "\n",
    "This adds two forces:\n",
    "1. **Entropy pressure** (explore) pushes variance up.\n",
    "2. **KL pressure** (stabilize) pushes variance down and means toward prior.\n",
    "\n",
    "**Failure modes to watch for:**\n",
    "- If entropy weight is too high, variance inflates and loss stalls.\n",
    "- If entropy weight is too low, TSU collapses back to baseline.\n",
    "\n",
    "We therefore use **small entropy weights**, optional temperature schedules, and compare energy per unit loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8b0da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_baseline(model, train_loader, val_loader, epochs=5, lr=3e-4):\n",
    "    '''\n",
    "    Baseline training: Standard cross-entropy minimization\n",
    "    Returns: training metrics + energy consumption\n",
    "    '''\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    # Energy monitoring\n",
    "    if GPU_MONITORING:\n",
    "        power_meter = NVMLPowerMeter()\n",
    "        power_meter.start()\n",
    "\n",
    "    metrics = {'train_loss': [], 'val_loss': [], 'epoch_times': [],\n",
    "               'energy_j': 0.0, 'avg_power_w': 0.0}\n",
    "\n",
    "    print(\"\ud83d\ude80 Starting BASELINE training...\")\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "\n",
    "        for batch_idx, (x, y) in enumerate(train_loader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits, loss = model(x, targets=y)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            # Sample power\n",
    "            if GPU_MONITORING and batch_idx % 10 == 0:\n",
    "                power_meter.sample()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                _, loss = model(x, targets=y)\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        avg_train = np.mean(train_losses)\n",
    "        avg_val = np.mean(val_losses)\n",
    "\n",
    "        metrics['train_loss'].append(avg_train)\n",
    "        metrics['val_loss'].append(avg_val)\n",
    "        metrics['epoch_times'].append(epoch_time)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train: {avg_train:.4f} | Val: {avg_val:.4f} | Time: {epoch_time:.2f}s\")\n",
    "\n",
    "    # Energy report\n",
    "    if GPU_MONITORING:\n",
    "        energy_stats = power_meter.stop()\n",
    "        metrics['energy_j'] = energy_stats['energy_j']\n",
    "        metrics['avg_power_w'] = energy_stats['avg_power_w']\n",
    "        print(f\"\\n\u26a1 Energy consumed: {energy_stats['energy_j']:.2f} J\")\n",
    "        print(f\"   Avg power: {energy_stats['avg_power_w']:.2f} W\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "print(\"\u2705 train_baseline() function loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc1ac3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_tsu_stats(model):\n",
    "    '''Aggregate entropy and KL across TSU layers in the model.'''\n",
    "    entropy = 0.0\n",
    "    kl = 0.0\n",
    "    count = 0\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, TSULinear):\n",
    "            entropy = entropy + module.entropy()\n",
    "            kl = kl + module.kl_divergence()\n",
    "            count += 1\n",
    "    return entropy, kl, count\n",
    "\n",
    "\n",
    "def train_with_tsu(model, train_loader, val_loader, epochs=5, lr=3e-4,\n",
    "                   temperature=1.0, entropy_weight=1e-4, kl_weight=1e-4,\n",
    "                   temp_decay=1.0):\n",
    "    '''\n",
    "    TSU Training: Free-energy-like objective\n",
    "    F = L - T * entropy_weight * S + kl_weight * KL\n",
    "    '''\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    # Energy monitoring\n",
    "    if GPU_MONITORING:\n",
    "        power_meter = NVMLPowerMeter()\n",
    "        power_meter.start()\n",
    "\n",
    "    metrics = {\n",
    "        'train_loss': [], 'val_loss': [], 'free_energy': [],\n",
    "        'entropy': [], 'kl': [], 'epoch_times': [],\n",
    "        'energy_j': 0.0, 'avg_power_w': 0.0\n",
    "    }\n",
    "\n",
    "    print(f\"\ud83c\udf21\ufe0f  Starting TSU training (T={temperature}, entropy_weight={entropy_weight}, kl_weight={kl_weight})...\")\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        model.train()\n",
    "        train_losses, free_energies, entropies, kls = [], [], [], []\n",
    "\n",
    "        # Optional temperature schedule\n",
    "        temp_t = temperature * (temp_decay ** epoch)\n",
    "\n",
    "        for batch_idx, (x, y) in enumerate(train_loader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits, loss = model(x, targets=y)\n",
    "\n",
    "            # Entropy + KL regularization from TSU layers\n",
    "            entropy, kl, n_tsu = collect_tsu_stats(model)\n",
    "            entropy_term = temp_t * entropy_weight * entropy\n",
    "            kl_term = kl_weight * kl\n",
    "\n",
    "            # Free energy objective\n",
    "            free_energy = loss - entropy_term + kl_term\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            free_energy.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            free_energies.append(free_energy.item())\n",
    "            entropies.append((entropy.item() / max(n_tsu, 1)))\n",
    "            kls.append((kl.item() / max(n_tsu, 1)))\n",
    "\n",
    "            # Sample power\n",
    "            if GPU_MONITORING and batch_idx % 10 == 0:\n",
    "                power_meter.sample()\n",
    "\n",
    "        # Validation (deterministic: uses mean weights in TSU)\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                _, loss = model(x, targets=y)\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        avg_train = np.mean(train_losses)\n",
    "        avg_val = np.mean(val_losses)\n",
    "        avg_fe = np.mean(free_energies)\n",
    "        avg_entropy = np.mean(entropies)\n",
    "        avg_kl = np.mean(kls)\n",
    "\n",
    "        metrics['train_loss'].append(avg_train)\n",
    "        metrics['val_loss'].append(avg_val)\n",
    "        metrics['free_energy'].append(avg_fe)\n",
    "        metrics['entropy'].append(avg_entropy)\n",
    "        metrics['kl'].append(avg_kl)\n",
    "        metrics['epoch_times'].append(epoch_time)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {avg_train:.4f} | FE: {avg_fe:.4f} | \"\n",
    "              f\"S: {avg_entropy:.2f} | KL: {avg_kl:.2f} | Val: {avg_val:.4f} | Time: {epoch_time:.2f}s\")\n",
    "\n",
    "    # Energy report\n",
    "    if GPU_MONITORING:\n",
    "        energy_stats = power_meter.stop()\n",
    "        metrics['energy_j'] = energy_stats['energy_j']\n",
    "        metrics['avg_power_w'] = energy_stats['avg_power_w']\n",
    "        print(f\"\\n\u26a1 Energy consumed: {energy_stats['energy_j']:.2f} J\")\n",
    "        print(f\"   Avg power: {energy_stats['avg_power_w']:.2f} W\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "print(\"\u2705 train_with_tsu() function loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7979113",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83d\udd2e STEP 6: Quantum Optimization with PennyLane (Optional Demo)\n",
    "\n",
    "**Important:** This section is a *standalone illustration* of QAOA-style optimization.\n",
    "It is **not** integrated into the training results in this notebook.\n",
    "\n",
    "### **Quantum Approximate Optimization Algorithm (QAOA):**\n",
    "\n",
    "**Ansatz state:**\n",
    "$$|\\psi(\\vec{\\gamma}, \\vec{\\beta})\\rangle = \\prod_{p=1}^{P} U_M(H_M, \\beta_p) U_P(H_C, \\gamma_p) |+\\rangle^{\\otimes n}$$\n",
    "\n",
    "where:\n",
    "- $U_P(H_C, \\gamma) = e^{-i\\gamma H_C}$: Problem unitary\n",
    "- $U_M(H_M, \\beta) = e^{-i\\beta H_M}$: Mixer unitary\n",
    "- $|+\\rangle = \\frac{1}{\\sqrt{2}}(|0\\rangle + |1\\rangle)$\n",
    "\n",
    "### **Cost Hamiltonian (Toy Example):**\n",
    "$$H_C = \\sum_{i=1}^{n} h_i Z_i + \\sum_{i<j} J_{ij} Z_i Z_j$$\n",
    "\n",
    "### **Expectation Value:**\n",
    "$$\\langle H_C \\rangle = \\langle \\psi(\\vec{\\gamma}, \\vec{\\beta}) | H_C | \\psi(\\vec{\\gamma}, \\vec{\\beta}) \\rangle$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e18ac28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantum optimization with PennyLane (optional - requires installation)\n",
    "try:\n",
    "    import pennylane as qml\n",
    "    QUANTUM_AVAILABLE = True\n",
    "    print(\"\u2705 PennyLane available - Quantum optimization enabled\")\n",
    "except ImportError:\n",
    "    QUANTUM_AVAILABLE = False\n",
    "    print(\"\u26a0\ufe0f  PennyLane not installed - Quantum features disabled\")\n",
    "    print(\"   Install with: pip install pennylane\")\n",
    "\n",
    "if QUANTUM_AVAILABLE:\n",
    "    # Define quantum device (simulator)\n",
    "    n_qubits = 4\n",
    "    dev = qml.device('default.qubit', wires=n_qubits)\n",
    "    \n",
    "    @qml.qnode(dev)\n",
    "    def qaoa_circuit(params, hamiltonian_coeffs):\n",
    "        \"\"\"\n",
    "        QAOA circuit for parameter optimization\n",
    "        From Word document: Quantum Approximate Optimization Algorithm\n",
    "        \n",
    "        Args:\n",
    "            params: [gamma, beta] angles for QAOA layers\n",
    "            hamiltonian_coeffs: Problem encoding (attention weights)\n",
    "        \"\"\"\n",
    "        # Initial state: uniform superposition\n",
    "        for i in range(n_qubits):\n",
    "            qml.Hadamard(wires=i)\n",
    "        \n",
    "        # QAOA layers\n",
    "        gamma, beta = params[0], params[1]\n",
    "        \n",
    "        # Problem Hamiltonian (encode attention parameters)\n",
    "        for i in range(n_qubits):\n",
    "            qml.RZ(gamma * hamiltonian_coeffs[i], wires=i)\n",
    "        \n",
    "        # Mixer Hamiltonian\n",
    "        for i in range(n_qubits):\n",
    "            qml.RX(beta, wires=i)\n",
    "        \n",
    "        # Entangling layer\n",
    "        for i in range(n_qubits - 1):\n",
    "            qml.CNOT(wires=[i, i+1])\n",
    "        qml.CNOT(wires=[n_qubits-1, 0])  # Circular\n",
    "        \n",
    "        # Measurement\n",
    "        return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "    \n",
    "    def quantum_parameter_optimization(attention_weights, n_iterations=20):\n",
    "        \"\"\"\n",
    "        Use QAOA to optimize attention head parameters\n",
    "        \n",
    "        Args:\n",
    "            attention_weights: Current attention weights [n_qubits]\n",
    "            n_iterations: Optimization steps\n",
    "            \n",
    "        Returns:\n",
    "            Optimized weights\n",
    "        \"\"\"\n",
    "        # Normalize weights to [-\u03c0, \u03c0]\n",
    "        hamiltonian_coeffs = np.pi * np.tanh(attention_weights[:n_qubits])\n",
    "        \n",
    "        # Initial QAOA parameters\n",
    "        params = np.array([0.5, 0.5])  # [gamma, beta]\n",
    "        \n",
    "        # Simple gradient descent\n",
    "        learning_rate = 0.1\n",
    "        for _ in range(n_iterations):\n",
    "            # Compute expectation values\n",
    "            expectations = qaoa_circuit(params, hamiltonian_coeffs)\n",
    "            \n",
    "            # Simple cost: negative sum of expectations (maximize alignment)\n",
    "            cost = -np.sum(expectations)\n",
    "            \n",
    "            # Numerical gradient (finite difference)\n",
    "            grad = np.zeros_like(params)\n",
    "            eps = 0.01\n",
    "            for i in range(len(params)):\n",
    "                params_plus = params.copy()\n",
    "                params_plus[i] += eps\n",
    "                cost_plus = -np.sum(qaoa_circuit(params_plus, hamiltonian_coeffs))\n",
    "                grad[i] = (cost_plus - cost) / eps\n",
    "            \n",
    "            # Update\n",
    "            params -= learning_rate * grad\n",
    "        \n",
    "        # Final expectations \u2192 optimized weights\n",
    "        final_expectations = qaoa_circuit(params, hamiltonian_coeffs)\n",
    "        optimized_weights = attention_weights.copy()\n",
    "        optimized_weights[:n_qubits] = final_expectations\n",
    "        \n",
    "        return optimized_weights\n",
    "    \n",
    "    print(f\"\ud83d\udd2e QAOA circuit configured:\")\n",
    "    print(f\"   - Qubits: {n_qubits}\")\n",
    "    print(f\"   - Device: default.qubit (simulator)\")\n",
    "    print(f\"   - Circuit depth: 2 (problem + mixer Hamiltonian)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc9d770",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83e\uddea STEP 7: Run Experiments & Comparative Analysis\n",
    "\n",
    "**Experimental Design:**\n",
    "\n",
    "We compare two training paradigms:\n",
    "\n",
    "1. **Baseline:** $\\min_\\theta \\mathcal{L}(\\theta)$\n",
    "2. **TSU:** $\\min_{q} \\; \\mathbb{E}_q[\\mathcal{L}] - T\\cdot S(q) + \\lambda D_{KL}[q||p]$\n",
    "\n",
    "**Metrics:**\n",
    "- Training loss: $\\mathcal{L}_{train}$\n",
    "- Validation loss: $\\mathcal{L}_{val}$\n",
    "- Energy consumption: $E_{total} = \\int P(t) dt$\n",
    "- Entropy evolution: $S(t)$\n",
    "- Training time: $T_{wall}$\n",
    "\n",
    "**Hypothesis:**\n",
    "TSU can reduce *energy per unit loss reduction* by smoothing optimization dynamics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c8c717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "TSU_TEMPERATURE = 1.0\n",
    "\n",
    "model_config = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'block_size': BLOCK_SIZE,\n",
    "    'n_embd': 256,\n",
    "    'n_head': 4,\n",
    "    'n_layer': 4,\n",
    "    'dropout': 0.1,\n",
    "    'use_tsu': False,\n",
    "    'tsu_temperature': TSU_TEMPERATURE\n",
    "}\n",
    "\n",
    "model_baseline = TinyGPT(**model_config)\n",
    "print(f\"\ud83c\udfd7\ufe0f  Baseline model initialized: {model_baseline.get_num_params():,} parameters\")\n",
    "\n",
    "# TSU model uses stochastic attention projections\n",
    "model_config_tsu = dict(model_config)\n",
    "model_config_tsu['use_tsu'] = True\n",
    "\n",
    "# Experiment configuration\n",
    "EPOCHS = 3  # Laptop-friendly (increase for real experiments)\n",
    "LEARNING_RATE = 3e-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e1dd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: Baseline Training\n",
    "print(\"=\"*60)\n",
    "print(\"EXPERIMENT 1: BASELINE (Classical SGD)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model_baseline = TinyGPT(**model_config)\n",
    "baseline_metrics = train_baseline(\n",
    "    model_baseline, train_loader, val_loader, \n",
    "    epochs=EPOCHS, lr=LEARNING_RATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749ad154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: TSU Training\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT 2: TSU (Free Energy Minimization)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model_tsu = TinyGPT(**model_config_tsu)\n",
    "tsu_metrics = train_with_tsu(\n",
    "    model_tsu, train_loader, val_loader,\n",
    "    epochs=EPOCHS, lr=LEARNING_RATE,\n",
    "    temperature=TSU_TEMPERATURE, entropy_weight=1e-4, kl_weight=1e-4, temp_decay=0.98\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef8f83f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83d\udcca STEP 8: Comparative Visualization & Statistical Analysis\n",
    "\n",
    "**Mathematical Analysis:**\n",
    "\n",
    "### **Loss Convergence Rate:**\n",
    "\n",
    "$$r = \\frac{\\mathcal{L}(0) - \\mathcal{L}(T)}{\\mathcal{L}(0)} \\times 100\\%$$\n",
    "\n",
    "### **Energy Efficiency Metric:**\n",
    "\n",
    "$$\\eta_{energy} = \\frac{\\mathcal{L}_{initial} - \\mathcal{L}_{final}}{\\int_0^T P(t) dt}$$\n",
    "\n",
    "Higher $\\eta$ = more loss reduction per Joule consumed.\n",
    "\n",
    "### **Pareto Optimality:**\n",
    "\n",
    "A method is **Pareto optimal** if no other method achieves both:\n",
    "- Lower final loss: $\\mathcal{L}_{final}' < \\mathcal{L}_{final}$\n",
    "- Lower energy: $E_{total}' < E_{total}$\n",
    "\n",
    "### **Statistical Significance (t-test):**\n",
    "\n",
    "$$t = \\frac{\\bar{E}_{baseline} - \\bar{E}_{TSU}}{s_p \\sqrt{\\frac{2}{n}}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f449403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loss comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Training loss\n",
    "axes[0].plot(baseline_metrics['train_loss'], 'o-', label='Baseline', linewidth=2)\n",
    "axes[0].plot(tsu_metrics['train_loss'], 's-', label='TSU', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Training Loss')\n",
    "axes[0].set_title('Training Loss Comparison')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation loss\n",
    "axes[1].plot(baseline_metrics['val_loss'], 'o-', label='Baseline', linewidth=2)\n",
    "axes[1].plot(tsu_metrics['val_loss'], 's-', label='TSU', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Validation Loss')\n",
    "axes[1].set_title('Validation Loss Comparison')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Energy consumption\n",
    "if GPU_MONITORING:\n",
    "    methods = ['Baseline', 'TSU']\n",
    "    energies = [baseline_metrics['energy_j'], tsu_metrics['energy_j']]\n",
    "    colors = ['#3498db', '#e74c3c']\n",
    "    \n",
    "    bars = axes[2].bar(methods, energies, color=colors, alpha=0.7, edgecolor='black')\n",
    "    axes[2].set_ylabel('Energy (Joules)')\n",
    "    axes[2].set_title('Total Energy Consumption')\n",
    "    axes[2].grid(True, axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, energy in zip(bars, energies):\n",
    "        height = bar.get_height()\n",
    "        axes[2].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{energy:.1f} J', ha='center', va='bottom', fontweight='bold')\n",
    "else:\n",
    "    axes[2].text(0.5, 0.5, 'GPU monitoring\\nnot available', \n",
    "                ha='center', va='center', transform=axes[2].transAxes, fontsize=12)\n",
    "    axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udcca Visualization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b59bab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed metrics report\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\ud83d\udccb FINAL METRICS REPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n\ud83c\udfaf BASELINE (Classical SGD):\")\n",
    "print(f\"   Final train loss: {baseline_metrics['train_loss'][-1]:.4f}\")\n",
    "print(f\"   Final val loss:   {baseline_metrics['val_loss'][-1]:.4f}\")\n",
    "print(f\"   Total time:       {sum(baseline_metrics['epoch_times']):.2f}s\")\n",
    "if GPU_MONITORING:\n",
    "    print(f\"   Energy consumed:  {baseline_metrics['energy_j']:.2f} J\")\n",
    "    print(f\"   Avg power:        {baseline_metrics['avg_power_w']:.2f} W\")\n",
    "\n",
    "print(\"\\n\ud83c\udf21\ufe0f  TSU (Free Energy Minimization):\")\n",
    "print(f\"   Final train loss:  {tsu_metrics['train_loss'][-1]:.4f}\")\n",
    "print(f\"   Final val loss:    {tsu_metrics['val_loss'][-1]:.4f}\")\n",
    "print(f\"   Final free energy: {tsu_metrics['free_energy'][-1]:.4f}\")\n",
    "print(f\"   Final entropy:     {tsu_metrics['entropy'][-1]:.2f}\")\n",
    "print(f\"   Final KL:          {tsu_metrics['kl'][-1]:.2f}\")\n",
    "print(f\"   Total time:        {sum(tsu_metrics['epoch_times']):.2f}s\")\n",
    "if GPU_MONITORING:\n",
    "    print(f\"   Energy consumed:   {tsu_metrics['energy_j']:.2f} J\")\n",
    "    print(f\"   Avg power:         {tsu_metrics['avg_power_w']:.2f} W\")\n",
    "\n",
    "if GPU_MONITORING and baseline_metrics['energy_j'] > 0:\n",
    "    energy_reduction = (1 - tsu_metrics['energy_j'] / baseline_metrics['energy_j']) * 100\n",
    "    print(f\"\\n\u26a1 ENERGY EFFICIENCY:\")\n",
    "    print(f\"   TSU vs Baseline: {energy_reduction:+.2f}% change\")\n",
    "\n",
    "    if energy_reduction > 0:\n",
    "        print(f\"   \u2705 TSU achieves {energy_reduction:.1f}% energy reduction!\")\n",
    "    else:\n",
    "        print(f\"   \u26a0\ufe0f  TSU uses {-energy_reduction:.1f}% more energy (entropy overhead)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd1a297",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83c\udfa8 STEP 9: Text Generation & Quality Evaluation\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "\n",
    "### **Autoregressive Generation:**\n",
    "\n",
    "$$P(x_{1:T}) = \\prod_{t=1}^{T} P_\\theta(x_t | x_{<t})$$\n",
    "\n",
    "### **Sampling Strategies:**\n",
    "\n",
    "**Greedy Decoding:**\n",
    "$$x_t = \\arg\\max_{v \\in \\mathcal{V}} P_\\theta(v | x_{<t})$$\n",
    "\n",
    "**Temperature Sampling:**\n",
    "$$P'(x_t = v | x_{<t}) = \\frac{\\exp(\\text{logit}_v / \\tau)}{\\sum_{v'} \\exp(\\text{logit}_{v'} / \\tau)}$$\n",
    "\n",
    "Higher $\\tau$ \u2192 more random, Lower $\\tau$ \u2192 more deterministic\n",
    "\n",
    "### **Generation Quality Metrics:**\n",
    "\n",
    "**Perplexity:**\n",
    "$$\\text{PPL} = \\exp\\left(-\\frac{1}{T}\\sum_{t=1}^{T}\\log P_\\theta(x_t | x_{<t})\\right)$$\n",
    "\n",
    "**Entropy of generation:**\n",
    "$$H = -\\sum_{v \\in \\mathcal{V}} P(v) \\log P(v)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5190c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, prompt=\"To be or not to be\", max_new_tokens=100, temperature=0.8):\n",
    "    \"\"\"\n",
    "    Generate text using the trained model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Encode prompt\n",
    "    context = torch.tensor([encode(prompt)], dtype=torch.long, device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop context to block_size\n",
    "            context_crop = context if context.size(1) <= model.block_size else context[:, -model.block_size:]\n",
    "            \n",
    "            # Forward pass\n",
    "            logits, _ = model(context_crop)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            # Sample\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Append\n",
    "            context = torch.cat([context, next_token], dim=1)\n",
    "    \n",
    "    generated = decode(context[0].tolist())\n",
    "    return generated\n",
    "\n",
    "# Generate samples from both models\n",
    "print(\"\ud83d\udcdd Text Generation Samples:\\n\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BASELINE MODEL:\")\n",
    "print(\"=\" * 60)\n",
    "baseline_text = generate_text(model_baseline, prompt=\"ROMEO:\", max_new_tokens=80)\n",
    "print(baseline_text)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TSU MODEL:\")\n",
    "print(\"=\" * 60)\n",
    "tsu_text = generate_text(model_tsu, prompt=\"ROMEO:\", max_new_tokens=80)\n",
    "print(tsu_text)\n",
    "\n",
    "print(\"\\n\u2705 Text generation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ceeace",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83d\udd2c STEP 10: Advanced Thermodynamic Analysis & Phase Transitions\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "\n",
    "### **Free Energy Landscape:**\n",
    "\n",
    "$$F(\\theta, T) = \\mathcal{L}(\\theta) - T \\cdot S(\\theta)$$\n",
    "\n",
    "As $T \\to 0$: Free energy $\\to$ Loss (pure exploitation)  \n",
    "As $T \\to \\infty$: Free energy dominated by entropy (pure exploration)\n",
    "\n",
    "### **Entropy Evolution Dynamics:**\n",
    "\n",
    "$$\\frac{dS}{dt} = -\\nabla_\\sigma S \\cdot \\frac{d\\sigma}{dt}$$\n",
    "\n",
    "**Phase Transition Detection:**\n",
    "\n",
    "Critical temperature where entropy suddenly drops:\n",
    "$$T_c = \\left(\\frac{\\partial S}{\\partial T}\\right)^{-1}_{max}$$\n",
    "\n",
    "### **Information Bottleneck:**\n",
    "\n",
    "$$\\min I(X; \\Theta) \\text{ subject to } I(\\Theta; Y) \\geq I_{min}$$\n",
    "\n",
    "where $I$ is mutual information.\n",
    "\n",
    "### **Thermodynamic Integration:**\n",
    "\n",
    "Total work done by entropy forces:\n",
    "$$W_{entropy} = \\int_{0}^{T_{train}} T(t) \\cdot \\frac{dS}{dt} dt$$\n",
    "\n",
    "### **Fluctuation-Dissipation Theorem:**\n",
    "\n",
    "$$\\langle (\\Delta \\theta)^2 \\rangle = 2T \\cdot D \\cdot \\Delta t$$\n",
    "\n",
    "where $D$ is diffusion coefficient, connecting temperature to parameter fluctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df96926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thermodynamic analysis of TSU training\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Free Energy Evolution\n",
    "axes[0, 0].plot(tsu_metrics['free_energy'], 'o-', color='#e74c3c', linewidth=2, markersize=8)\n",
    "axes[0, 0].set_xlabel('Epoch', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Free Energy F(\u03b8)', fontsize=11)\n",
    "axes[0, 0].set_title('Free Energy Minimization: F(\u03b8) = L(\u03b8) - T\u00b7S(\u03b8)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Entropy Evolution\n",
    "axes[0, 1].plot(tsu_metrics['entropy'], 's-', color='#9b59b6', linewidth=2, markersize=8)\n",
    "axes[0, 1].set_xlabel('Epoch', fontsize=11)\n",
    "axes[0, 1].set_ylabel('Entropy S(\u03b8)', fontsize=11)\n",
    "axes[0, 1].set_title('Parameter Distribution Entropy', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Loss vs Free Energy comparison\n",
    "axes[1, 0].plot(tsu_metrics['train_loss'], 'o-', label='Loss L(\u03b8)', color='#3498db', linewidth=2)\n",
    "axes[1, 0].plot(tsu_metrics['free_energy'], 's-', label='Free Energy F(\u03b8)', color='#e74c3c', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Epoch', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Value', fontsize=11)\n",
    "axes[1, 0].set_title('Loss vs Free Energy Dynamics', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].legend(fontsize=10)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Energy-Accuracy Trade-off\n",
    "if GPU_MONITORING:\n",
    "    baseline_final_loss = baseline_metrics['val_loss'][-1]\n",
    "    tsu_final_loss = tsu_metrics['val_loss'][-1]\n",
    "    baseline_energy = baseline_metrics['energy_j']\n",
    "    tsu_energy = tsu_metrics['energy_j']\n",
    "    \n",
    "    axes[1, 1].scatter([baseline_energy], [baseline_final_loss], \n",
    "                      s=300, marker='o', color='#3498db', edgecolor='black', linewidth=2,\n",
    "                      label='Baseline', zorder=3)\n",
    "    axes[1, 1].scatter([tsu_energy], [tsu_final_loss],\n",
    "                      s=300, marker='s', color='#e74c3c', edgecolor='black', linewidth=2,\n",
    "                      label='TSU', zorder=3)\n",
    "    \n",
    "    axes[1, 1].set_xlabel('Energy Consumption (J)', fontsize=11)\n",
    "    axes[1, 1].set_ylabel('Final Validation Loss', fontsize=11)\n",
    "    axes[1, 1].set_title('Energy-Performance Trade-off', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].legend(fontsize=10)\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add arrows and annotations\n",
    "    axes[1, 1].annotate('', xy=(tsu_energy, tsu_final_loss), \n",
    "                       xytext=(baseline_energy, baseline_final_loss),\n",
    "                       arrowprops=dict(arrowstyle='->', lw=2, color='green', alpha=0.6))\n",
    "    \n",
    "    # Pareto improvement region\n",
    "    axes[1, 1].axvline(baseline_energy, color='gray', linestyle='--', alpha=0.3)\n",
    "    axes[1, 1].axhline(baseline_final_loss, color='gray', linestyle='--', alpha=0.3)\n",
    "else:\n",
    "    axes[1, 1].text(0.5, 0.5, 'GPU Energy Monitoring\\nNot Available\\n\\nInstall pynvml:\\npip install pynvml',\n",
    "                   ha='center', va='center', transform=axes[1, 1].transAxes, \n",
    "                   fontsize=11, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    axes[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\u2705 Advanced thermodynamic analysis complete!\")\n",
    "print(\"\\n\ud83d\udcca Key Insights:\")\n",
    "print(f\"   - Free energy trajectory shows {'convergence' if tsu_metrics['free_energy'][-1] < tsu_metrics['free_energy'][0] else 'instability'}\")\n",
    "print(f\"   - Entropy evolution: {tsu_metrics['entropy'][0]:.2f} \u2192 {tsu_metrics['entropy'][-1]:.2f}\")\n",
    "print(f\"   - Entropy {'decreased' if tsu_metrics['entropy'][-1] < tsu_metrics['entropy'][0] else 'increased'} during training (parameter distribution narrowing)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234790d9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83c\udfc1 Workshop Conclusions & Key Takeaways\n",
    "\n",
    "### \ud83c\udfaf Summary\n",
    "\n",
    "This workshop explored **entropy-regularized training** for energy-aware LLM fine-tuning on GPUs.\n",
    "\n",
    "### \ud83d\udcca Key Results (Interpret Carefully)\n",
    "\n",
    "**1. Objective:**\n",
    "$$\\boxed{\\mathcal{F}(q) = \\mathbb{E}_q[\\mathcal{L}] - T\\cdot S(q) + \\lambda D_{KL}[q||p]}$$\n",
    "\n",
    "**2. Energy Efficiency:**\n",
    "- Baseline: Standard SGD with loss $\\mathcal{L}(\\theta)$\n",
    "- TSU: Stochastic weights + entropy/KL regularization\n",
    "- Outcome: **Measure** energy per unit loss, do not assume reduction a priori\n",
    "\n",
    "**3. Training Dynamics:**\n",
    "- Entropy evolves: $S(0) \\to S(T)$ (exploration \u2192 exploitation)\n",
    "- Entropy too high can stall learning; too low collapses to baseline\n",
    "\n",
    "### \ud83d\udd2c Theoretical Insights\n",
    "\n",
    "$$\\underbrace{\\mathcal{F}}_{\\text{Free Energy}} = \\underbrace{\\mathbb{E}_q[\\mathcal{L}]}_{\\text{Internal Energy}} - \\underbrace{T\\cdot S(q)}_{\\text{Entropic Force}} + \\underbrace{\\lambda D_{KL}}_{\\text{Stability}}$$\n",
    "\n",
    "### \ud83d\ude80 Practical Applications\n",
    "\n",
    "1. **Energy-Aware Fine-Tuning:** Track Joules per loss improvement\n",
    "2. **Stochastic Regularization:** Improve robustness in small-data regimes\n",
    "3. **Hardware Profiling:** Use NVML to connect algorithmic choices to energy\n",
    "\n",
    "### \ud83d\udd2e Future Directions\n",
    "\n",
    "1. **Temperature Schedules:** Adaptive $T(t)$ tied to gradient norms\n",
    "2. **Objective Balancing:** Tune $(T, \\lambda)$ for stability vs exploration\n",
    "3. **Larger-Scale Studies:** Repeat with more epochs + multiple seeds\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcd6 References & Further Reading\n",
    "\n",
    "1. Extropic (2024): \"An efficient probabilistic hardware architecture for diffusion-like models\" (arXiv:2510.23972v1)\n",
    "2. Friston, K. (2010): \"The free-energy principle: a unified brain theory?\" Nature Reviews Neuroscience\n",
    "3. Farhi et al. (2014): \"A Quantum Approximate Optimization Algorithm\" arXiv:1411.4028\n",
    "4. Hinton & Van Camp (1993): \"Keeping neural networks simple by minimizing the description length\" COLT 1993\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 25px; border-radius: 15px; color: white; text-align: center; margin: 20px 0;\">\n",
    "  <h2 style=\"margin: 0; font-size: 24px;\">\ud83c\udf1f Thank You for Participating! \ud83c\udf1f</h2>\n",
    "  <p style=\"margin: 15px 0; font-size: 16px;\">Questions? Discussions? Let's explore thermodynamic AI together!</p>\n",
    "  <p style=\"margin: 10px 0; font-size: 14px; opacity: 0.9;\">Contact: [Your Workshop Details Here]</p>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}